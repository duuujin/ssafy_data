<!-- ---------------------- EDA ---------------------- -->
# EDA(Exploratory Data Analysis)
- 데이터를 탐색하고 가설을 세우고 증명하는 과정, 이해하고 특징을 찾아내는 과정
- 데이터 종류, 사용 모델에 따라 EDA 방향성이 다양함

# 탐색적 데이터 분석(EDA)
- 데이터의 본질을 이해하고, 데이터에 숨겨진 패턴을 찾기 위해 데이터를 시각화하고 요약하는 과정
- 데이터 시각화(Matplotlib, seaborn, ...)
- 가정에 얽매이지 않음(고전적 통계학에서는 주로 가정을 세움)
- 이상치 탐지(특이한 값)
- 기술 통계 수행(고전적 통계와 비슷)

# EDA 과정에서 파악할 요소
- 데이터 사이즈는 어느 정도인지
- 학습 데이터와 테스트 데이터는 어떻게 분리가 되어있는지
- 결측값은 어느 정도 인지
- 라벨이 있는 데이터라면 분포는 어떻게 되어있는지
- 데이터의 특이점이 있는지

# 수치형 데이터
- 연속형 데이터(무한한 값을 가질 수 있는 수치데이터)
    - 풍속, 지속시간 등

- 이산형 데이터(특정한 정수값을 가질 수 있는 수치데이터)
    - 사건의 발생 빈도 등

# 범주형 데이터
- 명목형 데이터(순서의 의미가 없는 범주데이터)
    - TV 스크린 종류(플라즈마, LCD, LED 등)
    - 도시명(대전,부산,서울 등)

- 순위형 데이터(순서의 의미가 있는 범주데이터)
    - 수치로 나타낼 수 있는 평점(1,2,3,4,5점 등)
    - 선호도(좋다, 중립, 싫다)

## 왜 굳이 데이터의 종류를 분류해야할까?
- 데이터를 분석하고 예측을 모델링할 때, 시각화, 해석, 모델 결정등에 데이터의 종류가 중요한 역할을 하기 때문

# 테이블 데이터
- 용어정리
    - DataFrame : 통계와 머신러닝 모델에서 가장 기본이 되는 테이블 형태의 데이터 구조
    - Feature : 일반적으로 테이블의 각 열을 의미
    - Output : 데이터 과학 프로젝트의 목표는 대부분은 어떤 결과를 예측하는 것에 있음. 그 결과를 예측하기 위해 피처를 사용
    - Record : 일반적으로 테이블에서 하나의 행을 의미

# 테이블 데이터가 아닌 데이터 구조
- 시계열 데이터
    - 동일한 변수 안에 연속적인 측정값을 갖는 데이터
    - ex : 주가, 기온, 센서 ....
- 공간데이터
    - 지도 제작과 위치 정보 분석에 사용되는 공간 데이터의 경우 테이블 데이터보다 좀 더 복잡하고 다양함
        - 객체를 표현할 때에는, 공간 좌표가 데이터의 중심이 됨
        - 필드 정보는 공간을 나타내는 작은 단위들과 적당한 측정 기준 값에 중점을 둠
- 그래프 데이터
    - 물리적 관계, 사회적 관계 그리고 다소 추상적인 관계들을 표현하기 위해 사용
    - 노드 연결 형태
        - 예시 : 페이스북이나 링크드인 값은 소셜네트워크의 그래프
        - 예시 : 도로로 연결된 물류 중심지

<!-- --------------인공지능 기본------------------------- -->
# 인공지능(AI) vs 머신러닝(ML) vs 딥러닝(DL)
1. 인공지능 : 규칙 기반의 알고리즘
2. 머신러닝 : 데이터 기반의 학습, 결정트리, 선형회귀, 퍼셉트론, SVM
3. 딥러닝 : CNN, RNN, GAN, 트랜스포머..

# 피처 엔지니어링이란
- 모델이 더 좋은 예측을 할 수 있도록 데이터를 가공하는 과정
- 데이터를 단순히 수집하는 것보다 적절한 피처(Feature, 변수)를 만드는 것이 성능 향상에 중요

# 피처 엔지니어링 필요성
- 원본 데이터는 모델이 바로 이해하기 어려운 형태일 수 있음
- 좋은 피처를 만들면 복잡한 모델 없이도 높은 성능을 낼 수 있음
- 불필요한 변수를 제거하면 연산 속도가 빨라지고 해석력이 향상됨

# 피처 선택(Feature Selection)
- 모델 성능에 중요한 변수만 선택하고 불필요한 변수 제거
- 방법 :
    - 상관계수 분석 -> 상관관계가 높은 변수 제거
    - 분산 기반 선택 -> 변화가 거의 없는 변수 제거
    - Recursive Feature Elimination(REF) -> 모델 성능에 가장 중요한 변수 선택

# 피처 생성(Feature Creation)
- 기존 데이터를 활용하여 새로운 변수를 만드는 과정
- 예시 :
    - 날짜 데이터 -> "요일", "주말 여부", "계절" 변수 추가
    - 판매 데이터 -> "가격" X "판매량"으로 매출 변수 생성

# 피처 변환(Feature Transformation)
- 모델이 데이터를 더 잘 학습할 수 있도록 변형
- 방법 :
    - 정규화(Normalization) : 값의 범위를 [0,1]로 조정
    - 표준화(Standardization) : 평균 0, 표준편차 1로 조정
    - 로그변환(Log Transformation) : 데이터 분포를 정규분포에 가깝게 변형

# 범주형 데이터 인코딩(Categorical Encoding)
- 문자 데이터를 숫자로 변환하여 모델이 이해할 수 있도록 함
- 방법 : 
    - 원-핫 인코딩(One-Hot Encoding) : 카테고리를 0과 1로 변환
    - 라벨 인코딩(Label Encoding) : 카테고리를 숫자로 변환(예: "Red" -> 1, "Blue" -> 2)

# 결측값 처리(Handling Missing Values)
- 데이터에서 비어있는 값(결측치)을 처리하는 과정
- 방법 :
    - 삭제 : 결측치가 적으면 해당 행 제거
    - 대체 : 평균, 중앙값, 최빈값으로 채우기
    - 예측 모델 사용 : 머신러닝을 활용하여 결측값 예측

# 이상치 처리(Outlier Handling)
- 극단적인 값을 감지하고 처리하는 과정
- 방법 :
    - Z-score, IQR(사분위 범위) 기반 이상치 제거
    - 로그 변환으로 이상치 영향을 줄이기

# 머신러닝의 분류
- Supervised Learning(지도 학습)
- Unsupervised Learning(비지도 학습)
- Reinforcement Learning(강화 학습)

## Supervised Learning(지도 학습)
- 정답을 주고 학습 시킴

## 컴퓨터 비전에서의 Supervised Learning(지도학습)
- 객체 위치 지정 및 탐지
    - x = raw pixeld of the Image, y = the bounding boxes

## 자연어 처리에서의 Supervised Learning(지도학습)
- 기계 번역

## Unsupervised Learning(비지도 학습)
- 데이터셋에 라벨이 없음 : x1, ... , xn
- 목표(모호하게 제시됨) : 데이터에서 흥미로운 구조를 발견하는 것

## Self-Supervised Learning 자기지도학습
- Large Language Model
    - 대규모 언어 데이터셋을 통해 학습된 기계 학습 모델
    - 다양한 목적으로 활용 가능
- 자기 지도 학습
    - 지도 학습의 단점인 정답 데이터에 대한 한계를 극복 - 정답이 없는 데이터도 자체 데이터로 학습
    - 사전 학습(Pre-training)과 미세 조정(Fine-tuning)단계로 진행

# Reinforcement Learning(강화 학습)
- 순차적인 결정을 내리는 학습
    - 알고리즘은 데이터를 상호작용적으로 수집할 수 있음

<!-- -----------------------모델 성능평가--------------------------- -->
# Overfitting vs Underfitting
- Overfitting(과적합)
    - 모델이 학습 데이터에 지나치게 적합하여, 복잡한 패턴까지 모두 학습
    - 학습 데이터에서는 높은 성능, 하지만 새로운 데이터에서는 성능이 저하
- Underfitting(과소적합)
    - 모델이 충분히 학습되지 않아 중요한 패턴조차 학습하지 못함
    - 학습 데이터와 테스트 데이터 모두에서 성능 저조

# 평가(Evaluation)
- 실제값과 모델에 의해 예측된 값을 비교하여 두 값의 차이(오차)를 구하는 것
- (실제 값 - 예측값) = 0 이면, 오차가 없는 것으로 모델이 100% 성능을 보임
- But 성능이 100%인 모델은 현실적으로 힘들기 때문에, 오차를 구하여 어느 정도까지 오차를 허용할지 결정
- -> 과적합(Overfitting)을 방지하고 최적의 모델을 찾기 위해 실시

- 모델링의 목적 또는 목표 변수의 유형에 따라 다른 평가지표 사용

# 회귀 모델 평가 방법
- 하나의 목표를 가지고 다양한 회귀 모델을 만들어 테스트
- 그 중 어떤 모델이 가장 나은 모델인지 판단할 때 모델 평가지표 사용
- 실제 데이터와 예측 결과 데이터가 얼마나 비슷한지에 기반하지만, 단순하게 이것만 가지고 판단하기에는 무리가 있음
    - 평균 절대 오차(MAE, Mean Absolute Error)
    - 평균 절대 백분율 오차(MAPE, Mean Absolute Percentage Error)
    - 평균 제곱 오차(MSE, Mean Squared Error)
    - 평균 제곱근 오차(RMSE, Root Mean Squared Error)
    - 결정 계수(r^2 Scire)

## MAE(Mean Absolute Error)
- 실제 정답 값과 예측 값의 차이를 절대값으로 변환한 뒤 합산하여 평균을 구함
- 이상치를 고려하지 않는 경우에 주로 사용
- 값이 낮을수록 좋은 모델
- 실제 정답보다 낮게 예측했는지, 높게 했는지를 파악이 어려움
- 스케일에 의존적이기 때문에 모델마다 여러 크기가 동일해도 에러율은 동일하지 않음

# MAPE(Mean Absolute Percentage Error)
- MAE를 상대적 오차로 변환하여 데이터의 스케일에 영향을 덜 받도록 함
- 이상치를 고려하지 않는 경우에 주로 사용
- 값이 낮을수록 좋은 모델
- 실제 정답보다 낮게 예측했는지, 높게 했는지를 파악이 어려움
- 실제 정답이 0에 가까운 경우, 값이 발산할 수 있으므로 주의

# MSE(Mean Squared Error)
- 실제 정답 값과 예측 값의 차이를 제곱한 뒤 평균을 구함
- 시계열 데이터에서, 미분이 연속적이어야 할 때 주로 사용
- 값이 낮을수록 좋은 모델
- 이상치(outlier)에 매우 민감(큰 오차에 큰 패널티)
- 데이터에 이상치가 많다면 MSE 대신 MAE를 고려
- 스케일 의존적이므로 모델 비교 시 주의 필요

# RMSE(Root Mean Squared Error)
- MSE에 루트를 씌워서 에러를 제곱해서 생기는 값의 왜곡을 줄임
- 시계열 데이터에서, 미분이 연속적인 순간이 필요할 때 주로 사용
- 값이 낮을수록 좋은 모델
- 이상치에 대한 패널티가 여전히 존재 (MSE보다는 덜 민감)(데이터의 크기가 다르면 RMSE 값도 다르게 나와서 서로 다른 스케일의 모델을 비교하기 어려움)
- 데이터에 이상치가 많다면 MSE 대신 MAE(Mean Absolute Error)계열 사용

# R2 score = R squared
- 모델이 실제 데이터의 변동성을 얼마나 잘 설명하는지를 나타내는 지표
- 실제 값의 전체 변동성(SST) 중에서 모델이 설명할 수 있는 비율
- 1에 가까울 수록 좋은 모델

# 분류 모델 평가 방법
- 하나의 목표를 가지고 다양한 분류 모델을 만들어 테스트
- 그 중 어떤 모델이 가장 나은 모델인지 판단할 때 모델 평가지표 사용
- 회귀모형과 비슷하게 실제 데이터와 예측 결과 데이터가 얼마나 비슷한 지에 기반하지만, 단순하게 이것만 가지고 판단하기에는 무리가 있음
    - 정확도(Accuracy)
    - 오차행렬(Confusion Matrix)
    - 정밀도(Precision)
    - 재현율(Recall)
    - F1-스코어
    - ROC AUC

# 오차행렬(Confusion Matrix)
- 분류 모델이 예측한 결과와 실제 정답을 비교하여 성능을 평가하는 방법
- 특히 이진 분류(binary classification)문제에서 모델이 얼마나 정확하게 예측했는지를 분석하는데 유용
- TP (True Positive)
    - 실제로 참인 데이터를 모델이 참으로 예측한 경우
- TN (True Negative)
    - 실제로 거짓인 데이터를 모델이 거짓으로 예측한 경우
- FP (False Positive)
    - 실제로 거짓인데 모델이 참으로 예측한 경우
- FN (False Negative)
    - 실제로 참인데 모델이 거짓으로 예측한 경우

# 정확도(Accuracy)
- 전체 샘플 중에서 모델이 맞춘 비율
- 하지만 데이터가 불균형한 경우에는 부적절할 수 있음(예: 99% 건강한 데이터, 1% 암 환자)


# 정밀도(precision)
- 모델이 "참(Positive)"이라고 예측한 것 중 실제로 참인 비율
- FP(거짓을 참으로 예측하는 경우)를 줄이는데 초점
- 예: 스팸 필터에서 중요(스팸 아닌 메일을 스팸으로 분류하면 안 됨)


# 재현율(Recall)
- 실제 참(Positive) 중에서 모델이 맞춘 비율
- FN(참을 거짓으로 예측하는 경우)을 줄이는데 초점
- 예 : 암 진단 모델에서 중요(암 환자를 놓치면 안 됨)

# Precision과 Recall의 Trade off
- 재현율(Recall)이 중요한 경우 -> 실제 Positive(양성)를 놓치면 안 되는 상황
    - 암 진단 : 암 환자를 놓치면 치명적
    - 금융 사기 탐지 : 사기 거래를 놓치면 피해 발생
- 정밀도(Precision)이 중요한 경우 -> 실제 Negative(음성)를 잘못 positive(양성)으로 예측하면 문제가 되는 상황
    - 스팸 필터 : 중요한 이메일을 스팸으로 분류하면 안 됨
    - 의료 검사 : 불필요한 추가 검사(비용증가)

# Precision과 Recall의 Trade off
- 정밀도와 재현율은 반비례 관계
- 임계값(Threshold)을 조정하면 정밀도(Precision) 또는 재현율(Recall)을 조절할 수 있음
- 하지만 정밀도를 높이면 재현율이 낮아지고, 재현율을 높이면 정밀도가 낮아지는 관계

# 정밀도를 100%로 만드는 방법
- 확실한 경우만 positive로 예측하고 나머지는 모두 Negative로 예측
- 예 : 1000명 중 단 1명만 확실한 양성이라 판단하고 나머지는 모두 음성으로 처리
- 결과 : FP(거짓 양성) = 0 -> 정밀도 100%
- 문제 : 대부분의 양성을 놓칠 위험(재현율이 극도로 낮아짐)

# 재현율을 100%로 만드는 방법
- 모든 데이터를 Positive(양성)으로 예측
- 예 : 1000명 모두 양성이라 가정하면 실제 양성 30명은 모두 검출됨
- 결과 : FN(거짓 음성) = 0 -> 재현율 100%
- 문제 : 너무 많은 FP(거짓 양성) 발생 -> 정밀도 급락

-> 정밀도와 재현율을 극단적으로 높이면 성능이 왜곡됨
-> 업무에 따라 적절한 균형이 필요(F1-score 활용)

# F1-score
- 정밀도와 재현율의 조화 평균
- 데이터가 불균형할 때 유용
- 정밀도와 재현율 사이에서 균형을 맞춘 지표
- 정밀도와 재현율이 어느 한쪽으로 치우치지 않는 수치를 나타낼 때 높은 값을 가짐

# ROC 곡선(Receiver Operating Characteristic Curve)
- False Positive Rate(FPR) 변화에 따른 True Positive Rate(TPR) 변화를 나타내는 그래프
- X축 : False Positive Rate(FPR, 잘못 양성으로 예측한 비율)
- Y축 : True Positive Rate(TPR, 실제 양성을 맞춘 비율)
- 곡선이 왼쪽 위로 올라갈수록 좋은 모델

# AUC(Area Under Curve)
- ROC 곡선 아래 면적을 의미하며 1에 가까울수록 좋은 성능
- 일반적으로 의학 및 머신러닝의 이진 분류 평가 지표로 활용됨


# ROC 곡선과 AUC
- TPR(True Positive Rate, 재현율)
    - 실제 양성(Positive) 중에서 모델이 맞게 예측한 비율
    - 공식 : TPR = TP / (TP + FN)
    - 값이 클수록 좋은 모델(양성을 잘 찾아냄)
- FPR(False Positive Rate)
    - 실제 음성(Negative) 중에서 모델이 잘못 예측한 비율
    - 공식 : FPR = FP / (FP + TN)
    - 값이 작을수록 좋은 모델(불필요한 오탐을 줄임)
- ROC 곡선의 의미
    - X축 : FPR(False Positive Rate)
    - Y축 : TPR(Treu Positive Rate)
    - 곡선이 왼쪽 위로 갈수록 좋은 모델
    - 임계값(Threshold)에 따라 FPR과 TPR이 변화
- 결론 :
    - 좋은 모델일수록 ROC 곡선이 좌상단에 위치
    - AUC(ROC 아래 면적)가 1에 가까울수록 성능이 우수함

<!-- ------------벡터 정의----------------------------------------- -->
# 벡터의 정의
- 벡터는 선형대수학의 기본 요소로, 크기와 방향을 가짐
- 좌표 평면에서 점을 나타낼 때 (x,y)와 같은 형태로 표현
- 벡터는 머신러닝과 데이터 분석 등에서 데이터를 표현하는 기본 단위로 사용
- 파이썬에서는 숫자로 이루어진 배열(리스트)형태로 표현

# 벡터의 좌표
- 벡터는 좌표로 표현되며, 두 숫자가 원점에서의 이동을 나타냄
- 첫 번째 숫자는 x축 방향 이동, 두 번째 숫자는 y축 방향 이동
- 3차원에서는 z축이 추가됨

# 데이터를 표현하는 방법
- 스칼라 : 하나의 숫자(0차원)
- 벡터 : 여러 개의 숫자가 나열된 1차원 배열
- 행렬 : 숫자의 행과 열로 구성된 2차원 배열
- 텐서 : 3차원 이상을 포함하는 행렬보다 높은 다차원 배열

# 텐서의 활용
- 텐서는 공간을 수치적으로 표현하는 도구
- 데이터 분석에서는 벡터를 사용하여 여러 특성을 표현함
- 벡터를 이용해 공간 내 객체를 표현하고 조작하는데 활용
- 텐서를 활용하여 딥러닝, 머신러닝에 적용하여 데이터를 표현함
- 이를 활용해 복잡한 데이터와 패턴을 학습하고 분석하는데 사용

<!-- ---------------분류--------------------- -->
# 분류(Classification)란 무엇
- 분류의 정의
    - 분류는 입력 데이터가 여러 개의 카테고리 중 하나에 속하도록 지정하는 작업
    - 주로 지도학습 방식으로 이루어지며, 데이터의 특징을 기반으로 해당 데이터가 어느 범주에 속하는지를 예측
- 분류 문제의 목표
    - 학습 알고리즘은 함수 f을 생성하여 입력 벡터 x가 어떤 카테고리 y에 속하는지를 예측
    - 예를 들어, 이미지 인식에서 입력은 이미지의 픽셀 값으로 이루어지고, 출력은 이미지에 포함된 객체를 나타내는 카테고리 번호
- 분류 문제의 예시
    - 객체 인식 : 이미지 속 사물(예 : 음료수 종류)을 인식하여 해당 사물이 무엇인지 분류하는 작업
    - 얼굴 인식 : 사진 속 인물을 인식하고 자동으로 태그하는 기술로, 사용자와 컴퓨터 간의 자연스러운 상호작용을 가능하게 함

# 분류와 회귀의 차이
- 분류
    - 정의 : 데이터를 미리 정의된 카테고리(범주)로 분류하는 작업
    - 출력값 : 이산적인 값(카테고리 또는 레이블)
    - 에시 : 이메일이 스팸인지 아닌지 구분, 암 진단 여부(양성/음성) 분류, 이미지 속 개체 인식
- 회귀(Regression)
    - 정의 : 연속적인 숫자 값을 예측하는 작업
    - 출력 값 : 연속적인 실수 값
    - 예시 : 주택 가격 예측, 온도 변화 예측, 주식 시장 가격 예측
- 핵심 차이점
    - 분류는 결과가 '카테고리'로 나타나며, 이산적인 값으로 분류
    - 회귀는 결과가 '숫자'로 나타나며, 연속적인 값을 예측

# 분류 문제의 입력과 출력
- 입력 데이터(Features, 입력 변수)
    - 분류 문제에서 입력 데이터는 하나 이상의 특징(Feature)으로 이루어진 벡터로 표현
    - 특징 벡터
        - 예시 : 이미지 인식에서 이미지의 각 픽셀 값이 특징이 될 수 있음
        - 입력 데이터는 연속형(숫자) 또는 이상형(범주형) 값으로 구성될 수 있음
- 출력 데이터(Labels, 출력 변수)
    - 분류 문제의 출력은 데이터가 속한 클래스(범주)를 나타냄
    - 출력 레이블 : y
        - 예시 : 이진 분류에서 출력은 0(음성) 또는 1(양성)
        - 다중 클래스 분류에서는 y가 1,2,3 등 여러 클래스를 나타냄
- 입력과 출력의 관계
    - 분류 모델은 주어진 입력 x에 대해 적절한 출력 y를 예측하는 함수 f(x)를 학습
    - 예시 : f(x)는 특정 사진을 입력 받아 그것이 고양이인지, 개인지, 또는 새인지를 확인

<!-- 결정경계  -->
# 결정 경계란
- 분류 모델이 데이터를 분류하기 위해 공간을 나누는 경계선
- 입력 데이터를 특징 공간에서 서로 다른 클래스 영역으로 구분

# 결정 경계의 역할
- 각 클래스에 속하는 데이터 포인트를 구분하는 기준을 제공
- 새로운 데이터가 주어졌을 때, 그 데이터가 어느 클래스에 속하는지 결정

# 결정 경계의 예시
- 선형 결정 경계
    - 단순한 직선이나 평면으로 클래스 간의 경계를 나눔
    - 예 : 로지스틱 회귀, 선형 서포트 벡터 머신(SVM)
- 비선형 결정 경계
    - 더 복잡한 경계를 만들어 클래스 간의 경계를 나눔
    - 예 : 결정 트리, 랜덤 포레스트, 심층 신경망(Deep Neural Networks)

# 결정 경계 시각화
- 2차원 특징 공간에서는 결정 경계를 쉽게 시각화 가능
- 각 클래스 영역이 어떻게 나누어져 있는지를 시각적으로 표현하면 모델의 동작을 더 쉽게 이해할 수 있음

<!-- ------------결정트리--------------------- -->
# 결정트리
- 데이터를 feature에 따라 트리 구조로 분할하여 분류하거나 예측하는 알고리즘
- 각 노드는 특성의 조건을 기반으로 데이터를 분기하며, 최종 리프 노드는 예측 결과(클래스)를 나타냄

# 작동방식
1. 특성 선택 : 각 특성을 기준으로 데이터를 분할할 수 있는 최적의 조건을 찾음
2. 분할 : 데이터를 해당 특성 조건에 따라 좌우로 분기함
3. 리프 노드 도달 시 각 분할된 노드가 특정 클래스로 분류될 때까지 분할을 반복함


# 특성 선택 기준
- 지니 지수(Gini Index) 또는 엔트로피(Entropy)를 사용하여 분할의 순도를 측정
- 순도가 높을수록(한쪽 클래스로 치우칠수록) 해당되는 특성은 더 좋은 분할 기준(값이 작으면 좋음)

# 장점
- 해석이 용이 : 트리 구조로 시각화가 가능하며, 결과 해석이 직관적
- 비선형 데이터에도 적합 : 복잡한 결정 경계를 만들 수 있음
- 데이터 전처리 과정이 간단하며, 범주형 데이터도 다룰 수 있음

# 단점
- 트리가 너무 깊어지면 과적합(overfitting)의 위험이 있음
- 작은 변화에도 모델이 민감하게 반응할 수 있음
- 데이터가 많아질수록 성능 저하가 발생할 수 있음

# 사용 예시
- 고객 이탈 예측, 의사결정 지원 시스템, 의료 진단 등 다양한 분야에서 사용

<!-- ----------------------회귀--------------- -->
# 회귀(Regression)란 무엇인가?
- 회귀의 정의
    - 회귀는 연속적인 숫자 값을 예측하는 지도 학습의 한 유형
    - 입력 데이터를 기반으로 특정 연속적 목표 값(출력)을 예측하는 것이 목적
- 회귀 문제의 목표
    - 입력 변수(특징)를 사용하여 출력 값의 추세를 학습하고, 이를 기반으로 새로운 데이터에 대해 에측을 수행
- 회귀의 활용 예시
    - 주택 가격 예측 : 집의 크기, 위치, 연도 등의 정보를 바탕으로 가격을 에측
    - 주식 시장 분석 : 과거 주가 데이터를 바탕으로 미래 주가를 예측
    - 날씨 예측 : 기온, 습도, 풍속 등의 데이터를 사용해 내일의 기온을 예측
- 회귀와 분류의 차이점
    - 회귀는 연속적인 값을 예측하는 문제(예:온도, 가격)
    - 분류는 이산적인 범주를 예측하는 문제(예: 스팸/비스팸, 고양이/개)

# 회귀 문제의 예시
- 주택 가격 예측
    - 주택의 크기, 방 개수, 위치 등의 특징(입력 변수)을 기반으로 주택의 가격(출력 변수)을 에측하는 문제
    - 예시: 100m<sup>2</sup>의 아파트가 5억 원일 가능성을 예측
- 주식 시장 분석
    - 과거 주가 데이터, 거래량, 경제 지표 등을 바탕으로 미래 주가를 예측하는 문제
    - 예시 : 주식 X의 다음 달 가격을 예측
- 날씨 예측
    - 기온, 습도, 풍속 등의 환경 데이터를 바탕으로 다음 날의 기온을 예측
    - 예시 : 내일의 최고 기온을 25도로 예측
- 자동차 연비 예측
    - 자동차의 배기량, 마력, 중량 등의 데이터를 기반으로 연비를 예측
    - 예시 : 특정 차량이 1리터당 12km를 갈 가능성 예측
- 판매량 예측
    - 과거 판매 기록, 계절, 마케팅 투자 등을 통해 미래의 판매량을 예측
    - 예시 : 특정 제품이 다음 달에 1000개 판매될 가능성 예측

# 입력 변수(Features, 독립 변수)
- 회귀 문제에서 입력 변수는 예측에 필요한 다양한 특성을 의미
- 입력 변수는 하나일 수도 있고, 여러 개일 수도 있음
- 예시
    - 주택 가격 예측에서의 입력 변수 : 주택 크기, 위치, 방 개수 등
    - 자동차 연비 예측에서의 입력 변수 : 배기량, 중량, 마력 등

# 출력 변수(Target, 종속 변수)
- 회귀 문제의 출력은 예측하고자 하는 연속적인 값
- 출력 변수는 모델이 학습하여 에측하는 목표 값
    - 주택 가격 예측에서의 출력 변수 : 예측된 주택의 가격
    - 자동차 연비 예측에서의 출력 변수 : 1리터당 이동할 수 있는 거리(km/L)

# 모델의 목적
- 입력 변수(특성)와 출력 변수(목표 값) 간의 관계를 학습해, 새로운 입력이 주어졌을 때 적절한 출력 값을 예측
- 예시 : 주택의 크기와 위치 정보를 입력으로 받아 주택 가격을 예측

# 회귀
- Training Data를 이용해서 데이터의 특성과 상관 관계 등을 파악하고, 그 결과를 바탕으로 Training Data에 없는 미지의 데이터가 주어졌을 경우에, 그 결과를 연속적인(숫자) 값으로 예측하는 것
- 예시 : 공부시간과 시험 성적 관계


# 회귀의 종류
- 단순 선형회귀 : 한 개의 독립변수로 종속변수를 예측, 선형적(1차항 이상의 연산없음). 직선으로 표현
- 다중 선형회귀 : 두 개 이상의 독립변수로 종속변수를 예측, 선형적. 3차원 이상에서 평면으로 표현
- 다항 회귀 : 2차항 이상의 항으로 계산, 다항식
- 로지스틱 회귀 : 범주형 종속변수를 예측하는 분류 모델

<!-- ------------선형 회귀---------------------- -->
# 선형회귀란
- 입력 변수(특성)와 출력 변수(목표 값) 간의 선형 관계를 가정하여 예측하는 모델
- 데이터가 직선으로 표현될 수 있을 때, 가장 간단하고 직관적인 방법

## 모델의 수식
- y = wx + b
- 여기서는 y는 예측 값, x는 입력 값, w는 웨이트, b는 바이어스

## 예측 방법
- 주어진 데이터를 기반으로 최적의 직선(회귀선)을 찾아 입력 값에 대한 출력 값을 예측
- 모델은 오차를 최소화하는 방향으로 직선의 기울기 w와 절편 b를 학습

## 장점
- 해석이 용이하고 계산이 빠름
- 비교적 단순한 문제에 대해 좋은 성능 발휘

## 단점
- 입력 변수와 출력 변수 간의 관계가 비선형적일 경우 성능이 저하될 수 있음
- 과적합의 위험이 있으며, 다중 공선성이 있을 때 문제가 발생할 수 있음

## 사용예시
- 주택 가격 예측 : 주택 크기에 따른 가격 에측
- 광고 비용과 판매량의 관계 분석


<!-- --------------다중 회귀------------------ -->
# 다중 회귀
- 여러 개의 입력 변수(특성)을 사용하여 출력 변수를 예측하는 선형 회귀 모델
- 단일 입력 변수를 사용하는 선형 회귀와 달리, 다중 회귀는 다수의 변수를 고려하여 보다 정확한 예측

## 모델의 수식
- 여기서 y는 예측 값, x1, x2, ..., xn은 입력 변수들, w1, w2, ..., wn은 각 특성의 가중치, b는 바이어스

## 예측 방법
- 각 입력 변수에 해당하는 가중치를 곱한 후, 그 값을 모두 더하여 예측 값을 계산
- 모델은 모든 입력 변수의 영향을 고려하여 오차를 최소화하는 방향으로 가중치와 절편을 학습


<!-- ----------------다항회귀-------------------- -->
# 다항 회귀란
- 입력 변수와 출력 변수 간의 비선형 관계를 모델링하는 회귀 방법
- 선형 회귀와 달리, 입력 변수의 거듭제곱을 포함한 다향식을 사용하여 비선형 패턴을 학습

## 장점
- 데이터가 비선형적인 경우, 선형 회귀보다 더 나은 예측 성능을 보임
- 다양한 비선형 패턴을 유연하게 모델링 할 수 있음

## 단점
- 과적합의 위험이 큼
- 특히 다항 차수가 높아질수록 모델이 복잡해져 과적합 가능성이 증가
- 데이터가 부족할 경우, 모델이 안정적이지 않을 수 있음

## 사용 예시
- 곡선 패턴이 있는 데이터 예측 : 예를 들어, 온도 변화에 따른 전력 사용량 예측
- 주택 가격 예측에서, 방 개수와 가격 간의 비선형 관계 모델링

## 모델의 수식
- 여기서 y는 예측 값, x는 입력 변수, w1,w2,...,wn은 각 특성의 가중치, b는 바이어스

## 예측 방법
- 입력 값 x의 거듭제곱을 사용해 곡선 형태의 관계를 모델링
- 선형 회귀로는 설명할 수 없는 비선형 패턴을 더 잘 포착 가능

<!-- ----------로지스틱 회귀------------------- -->
# 로지스틱 회귀
- 이진 분류를 위한 대표적인 선형 모델
- 입력 데이터를 기반으로 특정 클래스에 속할 확률을 예측
- 회귀라는 이름이 붙었지만 분류 문제에 사용
- S자형 곡선을 가지는 함수

# 로지스틱 함수(Sigmoid 함수)
- 예측 값을 0과 1사이의 확률로 변환하는 함수

# 장점
- 해석이 간단하고 계산이 빠름
- 데이터가 선형적으로 분리될 수 있는 경우 좋은 성능

# 단점
- 비선형 데이터에는 적합하지 않을 수 있음
- 복잡한 결정 경계를 만들기 어려움

<!-- ---------KNN---------------- -->
# k-최근접 이웃 알고리즘(k-Nearest Neighbors, K-NN)
- 지도 학습에서 사용되는 간단한 분류 알고리즘
- 데이터 포인트가 주어졌을 때, 그 데이터와 가장 가까운 k개의 이웃을 기준으로 클래스를 결정

# 작동 방식
1. 거리 계산 : 새로운 데이터와 훈련 데이터 간의 거리를 계산
2. k개의 이웃 선택 : 가장 가까운 k개의 이웃을 선택
3. 다수결 투표 : 선택된 이웃 중 가장 많은 클래스에 속하는 클래스를 새로운 데이터의 클래스로 예측

# 하이퍼파라미터 k
- k 값은 미리 정해지는 값이며, k가 너무 작으면 과적합이 발생하고, k가 너무 크면 과소적합이 발생

# 장점
- 직관적이고 이해하기 쉬움
- 복잡한 결정 경계도 학습할 수 있음

# 단점
- 데이터가 많아지면 계산 비용이 많이듦
- 차원의 저주에 취약함
- 중요한 특징을 자동으로 학습하지 않으므로 Feature Engineering이 필요할 수 있음

# 사용 예시
- 이미지 인식, 추천 시스템, 패턴 인식 등에서 사용

<!-- ------------------나이즈 베이즈 분류------------- -->
# 나이즈 베이즈 분류
- 베이즈 정리(Bayes Rule)를 활용한 확률 통계학적 분류 알고리즘
- 각 특징들이 독립이라면 다른 분류 방식에 비해 결과가 좋고, 학습 데이터도 적게 필요
- 각 특징들이 독릭이 아니라면, 즉- 특징들이 서로 영향이 있다면 분류 결과 신뢰성 하락
- 학습 데이터에 없는 범주의 데이터일 경우 정상적 예측 불가능

<!-- -------------------SVM---------------------------- -->
# 서포트 벡터 머신(SVM)
- 서포트 벡터 머신은 여백(Margin)을 최대화하는 초평면(Hyperplane)을 찾는 지도 학습 알고리즘
- 딥러닝의 대두 이전까지 분류, 회귀에 모두 사용할 수 있는 매우 강력한 모델

## 작동방식
- 1. 초평면 결정 : 두 클래스 사이의 간격을 최대화하는 초평면을 찾음
- 2. 서포트 벡터 : 초평면에 가장 가까운 데이터 포인트들이 서포트 벡터 역할을 하며, 이들이 결정 경계를 형성
- 3. 비선형 데이터 처리 : 커널 트릭(Kernel Trick)을 사용하여 비선형 데이터를 고차원 공간으로 변환한 후, 선형 분리를 수행할 수 있음

## 장점
- 상대적으로 데이터의 이해도가 떨어져도 사용이 용이함
- 예측 정확도가 통상적으로 높음

## 단점
- C(에러에 부여하는 가중치)를 결정해야 함
- 파라미터의 결정과 모형의 구축에 시간이 오래걸림

## 여백의 의미
- 주어진 데이터가 오류를 발생시키지 않고 움직일 수 있는 최대공간
- 분류 문제와 회귀 문제 각각의 문제에 따라 정의가 다름

## 최적의 결정 경계
- 최적의 결정 경계는 데이터 군집으로부터 최대한 멀리 떨어지는 것

## 분류에서의 여백(Margin)
- 데이터가 2차원이며, 라벨은 파란색과 빨간색 두개의 클래스가 있다고 가정
- 이를 분류하는 초평면은 초록색 직선이고, 1번 데이터를 기준으로 해당 데이터가 직선과 수직으로 움직인다면 1번 데이터가 직선을 초과해 움직이면, 해당 데이터를 직선이 빨간색 데이터로 예측해 오류가 발생
- 즉, 파란색 1번 데이터와 직선과의 거리가 여백(Margin)임

## 회귀에서의 여백(Margin)
- 단순 선형 모델(설명 변수 1개)를 생성하고, 데이터들이 초평면으로 부터 e > 0 범위 내에 있다고 가정
- 데이터들은 초록색 직선으로 부터 e 범위(양쪽 점선 사이)에 있으며, 데이터들은 x축 상에서 움직임
- 양 점선 사이의 수평거리를 데이터가 넘어가게 되면 e 범위를 넘어가게 되어 오류를 발생시킴
- 즉, 양 점선 사이의 수평거리가 여백(Margin)임

# Soft Margin vs Hard Margin
- Sorft Margin
    - Soft Margin은 결정 경계를 조금씩 넘어가는 데이터들을 어느 정도 허용하여 유연한 결정 경계를 만듦
    - 아래와 같은 데이터 분포는 직선으로 두 클래스를 분류하기 어렵기 때문에 어느 정도의 비용(Cost, C)을 감수하면서 가장 최선의 결정 경계를 찾음
    - Cost는 모델링을 하면서 설정이 가능하고, 이 값이 크면 클수록 Hard Margin을, 작으면 작을수록 Soft Margin을 만듦

- Hard Margin
    - Hard Margin은 이상치들을 허용하지 않고, 분명하게 나누는 결정 경계를 만듦
    - 과적합(Overfitting)의 오류가 발생하기 쉬움
    - 노이즈로 인해 최적의 결정 경계를 잘못 설정하거나 못 찾는 경우가 발생할 수 있음

# Kernel Trick
- 데이터를 고차원으로 보내서 서포트 벡터를 구하고 다시 저차원으로 축소하는 과정은 복잡하고 많은 연산량이 필요하기 때문에 Kernel Trick을 사용
- Kernel Trick은 선형 분리가 불가능한 저차원 데이터를 고차원으로 보내 선형 분리를 하는 이론을 이용한 일종의 Trick 기법
- Kernel Trick은 고차원 Mapping과 고차원에서의 연산이 가능함

# 대표적인 Kernel 함수
- Linear : 선형함수
- Poly : 다항식 함수
- RBF : 방사 기저 함수
- Hyperbolic Tangent : 쌍곡선 탄젠트

<!-- -------------군집------------------------ -->
# 군집(Clustering)
- 군집 분석은 비슷한 특성을 가진 데이터를 그룹화하는 비지도 학습 기법
- 데이터의 내재된 구조를 발견하고, 패턴을 파악하는데 활용
- 마케팅 세분화, 이상치 탐지, 추천 시스템 등 다양한 분야에서 사용

# K-평균(K-means clustering)
- K-평균 알고리즘은 가장 대표적인 군집화 알고리즘
    - 1. 사용자가 지정한 k개의 군집 중심점을 초기화하고, 반복적으로 업데이트
    - 2. 각 데이터 포인트를 가장 가까운 군집 중심점에 할당하고, 군집 중심점을 재계산하는 과정을 수렴할 때까지 반복
- 간단하고 빠르지만, K 값을 사전에 지정해야 함

<!-- ------------앙상블------------------------ -->
# 의사결정 트리(Decision Tree)
- 의사결정 트리는 분류, 회귀 문제에 모두 상요할 수 있는 모델
- 의사결정 트리는 입력 변수를 특정한 기준으로 분기해 트리 형태의 구조로 분류하는 모델

## 장점
- 해석이 쉬움
- 입력 값이 주어졌을 때 설명 변수의 영역의 흐름을 따라 출력값이 어떻게 나오는지 파악하기 용이

## 단점
- 예측력이 떨어짐
- 단순히 평균 또는 다수결 법칙에 의해 예측을 수행
- 회귀모델에서 반응 변수의 평균을 예측값으로 추정할 때 평균을 사용해 이상치에 영향을 많이 받음

## 앙상블 학습(Ensemble Learning)
- 여러 개의 분류기를 생성하고, 그 예측을 결합함으로써 보다 정확한 예측을 도출하는 기법
- 강력한 하나의 모델을 사용하는 대신 보다 약한 모델 여러 개를 조합하여 더 정확한 예측에 도움을 주는 방식
- 보팅(Voting), 배깅(Bagging), 부스팅(boosting) 세 가지의 유형이 존재

## 보팅(Voting)
- 여러 개의 분류기가 투표를 통해 최종 예측 결과를 결정하는 방식
- 서로 다른 알고리즘을 여러 개 결합하여 사용

## 보팅 방식
- 하드 보팅(Hard Voting) : 다수의 분류기가 예측한 결과값을 최종 결과로 선정
- 소프트 보팅(Soft Voting) : 모든 분류기가 예측한 레이블 값의 결정 확률 평균을 구한 뒤 가장 확률이 높은 레이블 값을 최종 결과로 선정

## 배깅(Bootstrap AGGregatING, Bagging)
- 데이터 샘플링(Bootstrap)을 통해 모델을 학습시키고 결과를 집계(Aggregation)하는 방법으로 모두 같은 유형의 알고리즘 기반의 분류기를 사용
- 데이터 분할 시 중복을 허용
- 범주형 데이터는 다수결 투표 방식으로 결과 집계
- 연속형 데이터는 평균값 집계를 활용, 과적합(overfitting) 방지에 효과적

## 부스팅(Boosting)
- 여러 개의 분류기가 순차적으로 학습을 수행
- 이전 분류기가 예측이 틀린 데이터에 대해서 올바르게 예측할 수 있도록 다음 분류기에게 가중치(weight)를 부여하면서 학습과 예측을 진행
- 예측 성능이 뛰어나 앙상블 학습을 주도
- 보통 부스팅 방식은 배깅에 비해 성능이 좋지만, 속도가 느리고 과적합이 발생할 가능성이 존재하므로 상황에 따라 적절하게 사용해야 함
- 대표적인 부스팅 모듈 XGBoost, AdaBoost, Gradient Boosting

## 랜덤 포레스트(Random Forest)
- 여러 개의 결정 트리들을 임의적으로 학습하는 방식의 앙상블 방법
- 여러가지 학습기들을 생성한 후 이를 선형 결합하여 최종 학습기를 만드는 방법
- 기반 기술
    - 의사결정 트리 : 여러가지 요소를 기준으로 갈라지는 가지를 트리형태로 구성하여 분석하는 기법
        - 의사결정 트리의 한계
            - 학습 데이터에 따라 생성되는 결정 트리가 크게 달라져 일반화가 어려운 과적합 문제 발생
            - 계층적 접근방식으로, 중간에 에러 발생 시 하위 계층으로 에러
    - 앙상블 학습 : 주어진 데이터를 여러 모델로 학습하고 종합하여 정확도를 높이는 기법
    - 배깅(Bagging) : 같은 알고리즘으로 여러 개의 분류기를 만들어서 결합하는 앙상블 학습 기법
- 특징 :
    - 임의성 : 서로 조금씩 다른 특성의 트리들로 구성
    - 비상관화 : 각 트리들의 예측이 서로 연관되지 않음
    - 견고성 : 오류가 전파되지 않아 노이즈에 강함
    - 일반화 : 임의화를 통한 과적합 문제 극복
- 장점
    - 과적합이 잘 일어나지 않음
    - 결측치나 이상치에 강함
    - 의사결정나무 알고리즘에 기반한 기법이기 때문에 scaling, 정규화 과정이 필요 없음
    - 비선형적 데이터에 강함
    - 새로운 데이터가 들어와도 크게 영향을 받지 않음
- 단점
    - 수 많은 트리를 계산하기 때문에 학습 시간과 계산 연산량이 큼

<!-- ----------LangChain--------------------- -->
# LangChain
- ChatGPT 프로그램 안에서 벗어나 LLM의 기능을 나만의 코드(Javascript, Python)으로 가져와서 이를 자유자재로 사용할 수 있게 해주는 강력한 "프레임워크"
- LLM으로 하는 모든 것을 LangChain을 통해서 할 수 있음을 의미
    - 프롬프트 엔지니어링
    - RAG
    - Agent
    - 외부 LLM API 사용 및 Local LLM 구동
    - Moderation

- LLM : 초거대 언어모델, 생성 모델의 엔진과 같은 역할을 하는 핵심 구성요소
    - 예시 : GPT-4, PALM, LLAMA, Deepseek...
- Prompts : 초거대 언어모델에게 지시하는 명령문
    - 예시 : Prompt Templates, Chat Prompt Template, Example Selectors
- Index : LLM이 문서를 쉽게 탐색할 수 있도록 구조화 하는 모듈
    - 예시 : Document Loaders, Text Splitters
- Chain : LLM 사슬을 형성하여 연속적인 LLM 호출이 가능하도록 하는 핵심 구성 요소
    - 예시 : LLM Chain, Question Answering, Summarization
- Agents : LLM이 기존 Prompt Template으로 수행할 수 없는 작업을 가능케 하는 모듈
    - 예시 : Custom Agent, Custom MultiAction Agent

# AI Agent
- 사용자의 목표를 달성하기 위해 스스로 문제를 분석하고, 해결 가능한 작은 작업 단위로 분해(Plannig)한 뒤, 필요시 외부 툴이나 API를 활용하여 작업을 수행하며, 결과를 반복적으로 검토(Self-Reflection)하고 개선하는 시스템

<!-- ------------------RAG---------------------- -->
# RAG
- 입력 프롬프트와 검색 기반의 정보를 결합(증강)하여, 증강된 정보를 기반으로 답변을 생성하도록 하는 방식
- 사전 훈련된 모델을 특정 작업이나 데이터셋에 맞게 추가적으로 조정하는 방식

# RAG(Retrieval-Augmented Generation)
- Retrieval(검색) : 외부 데이터 및 소스를 검색하여 정보 획득
- Augmented(증강) : 사용자의 질문을 보강하여 보다 정확한 문맥 제공
- Generation(생성) : 향상된 정보를 기반으로 더 좋은 답변 생성
- 답변할 때 확실한 출처를 기반으로 생성하게 됨

# RAG 장점
- 환각 현상(Hallucination) 감소
- 도메인 적응성 개선
- Open domain QA 성능 향상
- 참고한 Knowledge base가 적절한지 판단 가능
- 정보 검색에 강함
![alt text](image-1.png)

# RAG 이해를 위한 이론
- 정보 검색(Retrieval)
    - 필요한 정보를 검색하는 작업
    - 데이터베이스, 인터넷, 또는 다른 정보 저장소에서 관련 정보를 찾아내는 과정
    - 사용자의 쿼리에 가장 잘 맞는 데이터를 식별하고 추출하는 기술과 알고리즘
    - 웹 검색 엔진, 디지털 도서관, 온라인 데이터베이스, 정보 검색 시스템 등 다양한 분야에서 중요한 역할 수행

- 역색인(Inverted index)
    - 색인 : 1-> 1페이지 호출, 100 -> 100페이지 호출
    - 각 데이터에 빠르게 접근할 수 있도록 도움
    - 역색인 : "학교" -> 3, 49, 100 페이지
    - 각 단어로 색인 정보를 연결 시켜 놓음으로 단어 기반 검색이 가능케 함

- TF-IDF
![alt text](image-2.png)

- BM25
    - TF-IDF의 정보검색에서의 단점을 보완
    - Q : 사용자가 입력한 쿼리
    - D : 대조해보려는 문서
    - 대부분의 텍스트 기반 검색을 진행할 때 가장 자주 쓰이는 방식
    ![alt text](image-3.png)

- BM25 정보 검색
- 유사도 알고리즘
![alt text](image-4.png)

- Sparse embedding
    - 대부분의 값이 , 몇몇 위치만 1인 벡터로 표현
    - 문장에 나오는 단어의 빈도를 기준으로 벡터를 만듦
    - Tf-Idf, BM25 등
    - 겹치는 단어가 있으면 유사도가 높게 나오지만 단어 간의 의미적인 관계를 포착하지 못함
- Dense Embedding
    - 의미를 나타내는 실수 값들로 이루어진 벡터로 표현
    - BERT와 같은 Pretrained Langhuage Model이 주로 사용 됨
![alt text](image-5.png)


- 상용 LLM
    - 텍스트 생성, 이해, 번역 등 다양한 NLP 작업을 수행
    - 매우 큰 텍스트 데이터 세트에서 학습
    - 높은 정확도와 자연스러운 언어 생성
    - ex : 제미나이, GPT

- RAG
    - 정보검색과 응답 생성을 결합한 모델
    - 사용자의 질문이 주어지면, Retriever는 관련된 정보나 문서를 데이터베이스에서 검색
    - 검색된 정보로 질문에 대한 답변 생성
    - 보다 풍부하고 정확한 정보를 제공 가능
    ![alt text](image-6.png)