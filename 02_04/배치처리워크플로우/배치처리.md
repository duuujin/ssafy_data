# 배치 처리(Batch processing)
- 일정량의 데이터를 모아서 한꺼번에 처리하는 방식
- 정해진 시간(예 : 매일 밤 12시) 또는 특정 이벤트(예:파일 업로드) 발생 시 실행
- 대량 데이터를 처리하는데 적합하며, 주로 ETL 파이프라인, 데이터 웨어하우스 적재 등에 활용
- 주요 활용 사례
    - 데이터 웨어하우스 적재(예 : Redshift, Snowflake)
    - 사용자 로그 분석(예 : 하루 단위 사용자 접속 로그 집계)
    - 기계 학습 모델 학습을 위한 데이터 준비

# 실시간 처리(Real-time Processing)
- 데이터가 발생하는 즉시 실시간으로 처리하는 방식
- 지연 시간이 짧고, 스트리밍 데이터(예 : 실시간 로그, IoT 센서 데이터)에 최적화
- 주로 실시간 모니터링, 이상 탐지, 실시간 추천 시스템 등에 활용

| 항목 | 배치 처리 | 실시간 처리 |
| :--- | :--- | :--- |
| 데이터 처리 방식 | 일정량의 데이터를 모아서 한 번에 처리 | 데이터가 들어오는 즉시 처리 |
| 처리 속도 | 느림(분~시간 단위) | 빠름(밀리초~초 단위) |
| 적용 사례 | 데이터 웨어하우스, 머신러닝 학습 데이터 준비 | 실시간 이상치 탐지, 실시간 추천 시스템, IoT데이터 분석 |
| 리소스 사용 | 주어진 시간에만 리소스 사용 | 지속적인 리소스 사용 |
| 비용 | 상대적으로 저렴(일정한 리소스 사용) | 고비용 |
| 사례 | 매일 오후 20시 로그 데이터 집계 | 실시간 주식 거래 분석 |


# Apache Spark
- 대규모 데이터를 빠르고 효율적으로 처리하는 분산 데이터 처리 프레임워크
- In-Memory Computing
    - Spark는 데이터를 메모리(RAM)에서 처리하기 때문에 디스크 I/O가 많은 Hadoop보다 훨씬 빠름
    - 메모리에서 데이터를 유지한 채 연산을 수행하여 반복 연산(예:머신러닝, 데이터 변환)이 Hadoop보다 최대 100배 빠름
- 다양한 데이터 처리 방식 지원
    - Spark는 단순한 배치 처리가 아니라 여러 가지 방식으로 데이터를 처리 할 수 있음
    - RDD(기본적인 Spark 데이터 구조)
    - DataFrame
    - Spark SQL
- Scalability
    - 수십~수천 대의 클러스터 노드에서 병렬 실행 가능
        - AWS, Azure, Google Cloud 환경에서도 손쉽게 확장 가능
- 배치 & 실시간 데이터 처리 모두 가능
    - Spark는 기본적으로 배치 처리를 지원하지만, 스트리밍 처리도 가능

# Spark Data Structure
- RDD(Resilient Distributed Dataset)
    - 분산된 데이터를 저장하고 처리하는 기본 단위(Hadoop의 HDFS와 유사)
    - 변경 불가능(Immutable) -> 안정적인 분산처리를 지원
    - 여러 노드에서 병렬로 처리 가능
- DataFrame(Pandas와 유사)
    - 구조화된 데이터(테이블 형태)를 처리하는 최적화된 데이터 구조
    - Spark SQL과 연동 가능 -> SQL 쿼리를 사용하여 데이터 변환
- Spark SQL
    - SQL을 활용해 데이터를 쉽게 조회하고 변환 가능
    - 다양한 데이터 소스(HDFS, S3, JDBC, Hive, Cassandra 등)에서 데이터를 가져올 수 있음

## Apache Spark의 RDD 배치 처리 과정
    1. 데이터를 parallelize 또는 외부 소스에서 불러와 RDD를 생성
    2. Transformation 연산(map, filter 등)은 RDD를 새로 생성하지만 즉시 실행되지는 않음
    3. 여러 Transformation이 체이닝되어 실행 계획(DAG)을 구성
    4. Action 연산이 호출될 때 DAG가 실행되어 실제 데이터 처리가 수행

## Apache Spark의 DataFrame 배치 처리 과정
    1. 다양한 데이터 소스에서 read() 또는 load()를 통해 DataFrame을 생성
    2. 생성된 DataFrame에 select(), filter(), groupby() 등의 Transformtaion 연산 적용
    3. 여러 단계의 Transformation이 체이닝되며, 새로운 DataFrame이 연속적으로 생성
    4. 마지막에 show(), count(), write()와 같은 Action이 호출되어 실제 실행이 이루어짐

# Apache Spark 배치 처리의 주요 활용 사례
- 데이터 웨어하우스 적재(ETL)
    - Spark를 활용해 데이터를 정제 & 변환 후 데이터 웨어하우스에 저장
    - ex) 매일 수집한 CSV 데이터를 Parquet 변환 후 Snowflake, BigQuery 적재
- 로그 데이터 분석
    - 웹사이트, 애플리케이션, 서버 로그 데이터를 분석하여 사용자 행동 분석
    - ex) 사용자의 클릭 로그를 분석하여 마케팅 전략 최적화
- 머신러닝 데이터 전처리
    - 대량의 데이터를 Spark에서 전처리하여 머신러닝 모델 학습에 사용
    - ex) 추천 시스템을 위한 사용자 행동 데이터 전처리

# Airfow에서 Spark를 활용
- 단순 Python 코드로 처리하기 어려운 대규모 데이터 처리
    - Pandas는 메모리에 로드할 수 있는 데이터 크기에 한계가 있음
    - 병렬 처리가 어려워 속도가 느림
    - 데이터를 여러 노드에서 나눠서 처리하는 기능이 없음
- Spark를 활용하여 대량 데이터 ETL 수행
    - 단순 SQL이나 Python으로는 복잡한 ETL 작업을 수행하기 어려움
    - 대규모 데이터 변환이 필요한 경우, Spark의 강력한 데이터 처리 기능이 필요
- Spark 작업을 모니터링하고 실패 시 자동 재시도할 수 있음
    - Spark 작업이 실패하면 Airflow에서 자동으로 감지하고 재시도 가능
    - Airflow Web UI에서 로그를 확인하고, 어떤 Task에서 문제가 발생했는지 쉽게 파악할 수 있음

# 배치 워크플로우에서 DAG의 중요성
- 자동화
    - 사람이 직접 실행할 필요 없이 정해진 스케줄에 따라 실행
- 유지보수 용이
    - 실행 로드 및 실패 이력을 기록하여 문제 해결 가능
- 확장성
    - 여러 작업을 DAG 내에서 관리하고, 병렬 실행 가능
- 재시도 및 오류 감지
    - Task 실패 시 자동으로 재시도하여 안정적인 운영 가능

