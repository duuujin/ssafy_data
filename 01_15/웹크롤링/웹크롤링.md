# 웹 크롤링이란?
- Web(거미줄) + Crawling(기어다니다)
- 크롤링
    - 인터넷 상의 웹 페이지를 자동으로 탐색하고 데이터를 수집하는 기술
- 스크래핑
    - 특정 웹페이지에서 원하는 데이터(텍스트, 이미지, 표 등)를 추출하는 과정
    
# 웹 크롤링의 특징
- 자동화
- 대량 데이터 수집
- 구조화된 데이터 추출

# 웹 데이터 수집 방법
- 웹 페이지를 가져와서 필요한 정보만 추출하기(스크래핑)
- WebDriver를 이용해 웹 브라우저 자동화하기(스크래핑 or 크롤링)
- 제공된 OpenAPI를 이용해 실시간으로 데이터 가져오기

# 웹 페이지에서 데이터 추출
- 웹페이지는 HTML(Hyper text markup language)을 중심으로 이루어져 있음
- 원하는 주소의 웹 페이지로 들어가 HTML 내용을 가져오고, 그 안에서 원하는 데이터가 어디 있는지 가져오는직업(parsing)
- Python에는 정적 페이지에서는 BeautifulSoup 라이브러리 주로 사용

# WebDriver로 웹 브라우저 자동화
- 크롬, 파이어폭스, IE등의 웹 브라우저에서 클릭, 텍스트 입력 등의 행동을 코드로 제어 할 수 있게끔 만든 브라우저를 WebDriver라고 함
- 로그인이 필요한 서비스 등 단순 URL만으로는 접속할 수 없는 세션 유지가 필요한 작업이나, 옵션이나 드롭다운 메뉴를 클릭해야함 데이터를 얻을 수 있는 작업에 필요함
    - 꼭 크롤링이 아니더라도, 웹 브라우저의 행동을 자동화하는 다른 작업(게시판에 자동 글쓰기, 새 메일 자동 읽기 등)
- Python에서는 Selenium 라이브러리를 사용함

# Robots.txt 이란?
- 웹사이트 소유자가 검색 엔진 크롤러에게 사이트의 특정 경로를 크롤링해도 되는지에 대한 권고 규칙을 전달하기 위해 사용하는 표준 텍스트 파일
- 검색 엔진 크롤러를 대상으로 한 권고 규칙
- 법적 강제력은 없으며 기술적 차단을 의미하는 것도 아님
- 검색 노출(인덱싱) 범위 제어 목적
- 스크래핑의 합법, 불법 기준은 아님
- robots.txt 위반만으로 불법이 되지는 않지만, 약관 위반, 과도한 수집 등과 결합될 경우 고의적인 자동 수집으로 판달되는 근거로 사용 될 수 있음

